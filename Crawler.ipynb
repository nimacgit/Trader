{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T07:55:47.243235Z",
     "start_time": "2019-08-30T07:55:41.151535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3\n",
      "  Using cached https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl\n",
      "Installing collected packages: urllib3\n",
      "Successfully installed urllib3-1.25.3\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install -q selenium bs4\n",
    "# !pip3 install jdatetime\n",
    "# !pip3 install khayyam\n",
    "!pip3 install -q urllib3\n",
    "# !pip install jupyter_contrib_nbextensions\n",
    "# !pip install nbresuse\n",
    "from selenium import webdriver\n",
    "from requests import Session, Request\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "import jdatetime\n",
    "from khayyam import JalaliDate, JalaliDatetime\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T07:55:48.873945Z",
     "start_time": "2019-08-30T07:55:48.843391Z"
    }
   },
   "outputs": [],
   "source": [
    "parentPageUrl = 'http://www.tsetmc.com/Loader.aspx?ParTree=15131F'\n",
    "saveLocation = \"../xcels/\"\n",
    "xcelBaseUrl = \"http://members.tsetmc.com/tsev2/excel/MarketWatchPlus.aspx?d=\"\n",
    "\n",
    "session = Session()\n",
    "request = Request(\"Get\", parentPageUrl)\n",
    "prepared = session.prepare_request(request)\n",
    "respond = session.send(prepared, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T07:55:49.538092Z",
     "start_time": "2019-08-30T07:55:49.522397Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch(saveLocation, year, month, day):\n",
    "    global xcelBaseUrl\n",
    "    \n",
    "    with open(saveLocation + str(year) + \"-\" + str(month) + \"-\" + str(day) + \".xlsx\", 'wb') as f:\n",
    "        xcelUrl = xcelBaseUrl + str(year) + \"/\" + str(month) + \"/\" + str(day)\n",
    "        xcelRequest = Request(\"Get\", xcelUrl)\n",
    "        xcelPrepared = session.prepare_request(xcelRequest)\n",
    "        xcelRespond = session.send(xcelPrepared, verify=False, stream=True)\n",
    "        xcelRespond.raise_for_status()\n",
    "        for chunk in xcelRespond.iter_content(chunk_size=8192): \n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "            # f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T07:55:50.503698Z",
     "start_time": "2019-08-30T07:55:50.491048Z"
    }
   },
   "outputs": [],
   "source": [
    "def crawl(startDate, endDate):\n",
    "    global saveLocation\n",
    "    \n",
    "    now = startDate\n",
    "    while now <= endDate:\n",
    "        jalaldate = JalaliDate(now)\n",
    "        year = jalaldate.year\n",
    "        month = jalaldate.month\n",
    "        day = jalaldate.day\n",
    "        if jalaldate.weekday() < 5 and not Path(saveLocation + str(year) + \"-\" + str(month) + \n",
    "                                                \"-\" + str(day) + \".xlsx\").is_file():\n",
    "            fetch(saveLocation=saveLocation, year=year, month=month, day=day)\n",
    "            time.sleep(5)\n",
    "        now = now + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T07:56:24.777968Z",
     "start_time": "2019-08-30T07:56:24.773698Z"
    }
   },
   "outputs": [],
   "source": [
    "# startDate = JalaliDate(year=1380, month=1, day=1).todate()\n",
    "# startDate = JalaliDate(year=1398, month=6, day=1).todate()\n",
    "# endDate = JalaliDate(year=1398, month=6, day=5).todate()\n",
    "# crawl(startDate, endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T07:56:25.403907Z",
     "start_time": "2019-08-30T07:56:25.334410Z"
    }
   },
   "outputs": [],
   "source": [
    "def crawlingThread(threadName, startDate):\n",
    "    endDateJalali = JalaliDate(year=1398, month=6, day=5)\n",
    "    while True:\n",
    "        if endDateJalali != JalaliDate.today():\n",
    "            endDateJalali = JalaliDate.today()\n",
    "            crawl(startDate, endDateJalali.todate())\n",
    "            startDate = endDateJalali.todate()\n",
    "            with open(saveLocation + \"crawlstat\", 'w') as statfile:\n",
    "                lastcheck = JalaliDatetime.now()\n",
    "                statfile.write(\"last check\")\n",
    "                statfile.write(str(lastcheck))\n",
    "                statfile.write(\"\\n\")\n",
    "                statfile.write(\"last crawl: \")\n",
    "                statfile.write(str(endDateJalali))\n",
    "                statfile.write(\"\\n\")\n",
    "        time.sleep(3600)\n",
    "\n",
    "startDate = JalaliDate(year=1398, month=6, day=1).todate()\n",
    "crawlThread = multiprocessing.Process(target=crawlingThread, args=(\"Thread-crawl\", startDate))\n",
    "crawlThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_object = open(\"user.pkl\", 'ab')\n",
    "# # pickle.dump(requests.utils.dict_from_cookiejar(resp.cookies), file_object)\n",
    "\n",
    "# soup = bs(resp.text, 'html.parser')\n",
    "# name = soup.find_all(itemprop='name')\n",
    "# name = [x.get_text() for x in name]\n",
    "# moredis = soup.findAll(\"p\", {\"class\": \"prd-desc more\"})\n",
    "# moredis = [x.get_text() for x in moredis]\n",
    "# weight = soup.findAll(\"span\", {\"class\": \"unit\"})\n",
    "# weight = [x.get_text() for x in weight]\n",
    "\n",
    "# import datetime\n",
    "# now = datetime.datetime.now()\n",
    "\n",
    "# with open('somefile') as f:\n",
    "#     cookies = requests.utils.cookiejar_from_dict(pickle.load(f))\n",
    "#     session = requests.session(cookies=cookies)\n",
    "\n",
    "# from http.cookies import SimpleCookie\n",
    "# rawdata = 'Cookie: _ga=GA1.2.969355752.1534482039;' \\\n",
    "#           ' JSESSIONID=nM0Rmitqyzs4sWMMdOBXB5BDbq09pS3ilLqwUJOV1bCZv_BxQgHS!484861232'\n",
    "# cookie = SimpleCookie()\n",
    "# cookie.load(rawdata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
