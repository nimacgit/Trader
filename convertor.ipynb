{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:13:15.666595Z",
     "start_time": "2019-10-10T14:13:15.639479Z"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T15:14:38.999094Z",
     "start_time": "2019-10-10T15:14:37.232980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing convertor.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile convertor.py\n",
    "\n",
    "# !pip3 install xlrd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import multiprocessing\n",
    "# !pip3 install khayyam\n",
    "from khayyam import JalaliDate, JalaliDatetime\n",
    "\n",
    "xcelLocation = \"../xcels/\"\n",
    "!export xcelLocation=\"../xcels/\"\n",
    "HEADER = [\"symbol\", \"name\", \"amount\", \"volume\", \"value\", \"lastday\", \"open\", \"close\",\n",
    " \"last-change\", \"last-percent\", \"ending\", \"ending-change\", \"ending-percent\",\n",
    " \"min\", \"max\",]\n",
    "HEADER_extra = HEADER + [\"year\", \"month\", \"day\", \"date\"]\n",
    "\n",
    "!ls $xcelLocation | grep \".xlsx\" > xlFiles\n",
    "tmp = !cat xlFiles\n",
    "names = [name[:-5] for name in tmp]\n",
    "\n",
    "\n",
    "def cleaner():\n",
    "    for name in names:\n",
    "        if os.path.getsize(xcelLocation + name + '.xlsx') < 10000:\n",
    "            os.remove(xcelLocation + name + '.xlsx')\n",
    "\n",
    "\n",
    "def convert(xcelLocation, xlFileName, returnAllMode=False):\n",
    "    xl = None\n",
    "    try:\n",
    "        if (not Path(xcelLocation + xlFileName + '.csv').is_file()) or returnAllMode:\n",
    "            xl = pd.read_excel(xcelLocation + xlFileName + \".xlsx\",\n",
    "                               header=[0], skiprows=[0,1], convert_float=False)\n",
    "            xl.columns = HEADER\n",
    "            xl.to_csv(xcelLocation + xlFileName + '.csv', encoding='utf-8', index=False, header=HEADER)\n",
    "    except:\n",
    "        xl = str(xlFileName)\n",
    "    finally:\n",
    "        return xl\n",
    "    \n",
    "def convertThread(threadname, q, qDFs, qErrors):\n",
    "    while not q.empty():\n",
    "        fileNames = q.get()\n",
    "        q.task_done()\n",
    "        for name in fileNames:\n",
    "            tmp = convert(xcelLocation=xcelLocation, xlFileName=name, returnAllMode=True)\n",
    "            if isinstance(tmp, str):\n",
    "                qErrors.put(tmp)\n",
    "            else:\n",
    "                qDFs.put((tmp.copy(), name))\n",
    "    print(str(threadname) + \" done\")\n",
    "\n",
    "def convert_all(batchSize=10, numThread=16):\n",
    "    all_dfs = []\n",
    "    all_df_names = []\n",
    "    i = 0\n",
    "    workers = []\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=numThread)\n",
    "    m = multiprocessing.Manager()\n",
    "    queue = m.Queue()\n",
    "    qDFs = m.Queue()\n",
    "    qErrors = m.Queue()\n",
    "\n",
    "    while i*batchSize < len(names):\n",
    "        if (i+1)*batchSize < len(names):\n",
    "            queue.put(names[i*batchSize:(i+1)*batchSize])\n",
    "        else:\n",
    "            queue.put(names[i*10:])\n",
    "        i+=1\n",
    "    print(len(names))\n",
    "    print(queue.qsize())\n",
    "\n",
    "    for i in range(numThread):\n",
    "    #     workers.append(Thread(target=readThread, args=(\"Thread-\" + str(i), queue, qsum, qcount)))\n",
    "    #     workers.append(pool.apply_async(readThread, (\"Thread-\" + str(i), queue, qsum, qcount,)))\n",
    "        workers.append(multiprocessing.Process(target=convertThread, args=(\"Thread-\" + str(i),\n",
    "                                                                        queue, qDFs, qErrors)))\n",
    "        workers[i].start()\n",
    "\n",
    "    for i in range(numThread):\n",
    "        workers[i].join()\n",
    "\n",
    "    while not qDFs.empty():\n",
    "        dftmp, nametmp = qDFs.get()\n",
    "        all_dfs.append(dftmp)\n",
    "        all_df_names.append(nametmp)\n",
    "\n",
    "    errors = []\n",
    "    while not qErrors.empty():\n",
    "       errors.append(qErrors.get())\n",
    "    print(len(all_dfs))\n",
    "    return all_dfs, all_df_names, errors\n",
    "\n",
    "def makeMasterTable(all_dfs, all_df_names, chunkSize):\n",
    "    for index, df in enumerate(all_dfs):\n",
    "        year, month, day = all_df_names[index].split(\"-\")\n",
    "        date = JalaliDate(year, month, day).todate()\n",
    "        yearlist = np.full(len(df), year).tolist()\n",
    "        monthlist = np.full(len(df), month).tolist()\n",
    "        daylist = np.full(len(df), day).tolist()\n",
    "        datelist = np.full(len(df), date).tolist()\n",
    "        df[\"year\"] = yearlist\n",
    "        df[\"month\"] = monthlist\n",
    "        df[\"day\"] = daylist\n",
    "        df[\"date\"] = datelist\n",
    "    xl = pd.concat(all_dfs, keys=all_df_names, ignore_index=True)\n",
    "    xl.columns = HEADER_extra\n",
    "    xl = xl.astype({\"year\": int, \"month\": int, \"day\": int})\n",
    "    xl['date'] = pd.to_datetime(xl['date'])\n",
    "    print(xl.dtypes)\n",
    "    xl.sort_values(by=['date'], inplace=True)\n",
    "    xl.reset_index(drop=True, inplace=True)\n",
    "    i = 0\n",
    "    while i*chunkSize < len(xl):\n",
    "        if (i+1)*chunkSize < len(xl):\n",
    "            df_i = xl.iloc[i*chunkSize:(i+1)*chunkSize]\n",
    "        else:\n",
    "            df_i = xl.iloc[i*chunkSize:]\n",
    "        df_i.to_csv('{xcelLocation}master{i}.csv'.format(i=i, xcelLocation=xcelLocation),\n",
    "                    header=HEADER_extra, encoding='utf-8', index=False)\n",
    "        i += 1\n",
    "    return xl\n",
    "\n",
    "def write_errors(errors):\n",
    "    with open(\"errors\", 'w') as error_file:\n",
    "    for error in errors:\n",
    "        error_file.write(str(error))\n",
    "        error_file.write(\"\\n\")\n",
    "\n",
    "        \n",
    "def error_cleaner():\n",
    "    for name in errors:\n",
    "        os.remove(xcelLocation + name + '.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T15:14:39.741028Z",
     "start_time": "2019-10-10T15:14:39.656031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4847\n",
      "485\n",
      "Thread-14 done\n",
      "Thread-8 done\n",
      "Thread-6 done\n",
      "Thread-15 done\n",
      "Thread-3 done\n",
      "Thread-12 done\n",
      "Thread-9 done\n",
      "Thread-10 done\n",
      "Thread-4 done\n",
      "Thread-1 done\n",
      "Thread-11 done\n",
      "Thread-7 done\n",
      "Thread-13 done\n",
      "Thread-5 done\n",
      "Thread-0 done\n",
      "Thread-2 done\n",
      "4846\n",
      "symbol                    object\n",
      "name                      object\n",
      "amount                   float64\n",
      "volume                   float64\n",
      "value                    float64\n",
      "lastday                  float64\n",
      "open                     float64\n",
      "close                    float64\n",
      "last-change              float64\n",
      "last-percent             float64\n",
      "ending                   float64\n",
      "ending-change            float64\n",
      "ending-percent           float64\n",
      "min                      float64\n",
      "max                      float64\n",
      "year                       int64\n",
      "month                      int64\n",
      "day                        int64\n",
      "date              datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "all_dfs, all_df_names, errors = convert_all()\n",
    "all_df = makeMasterTable(all_dfs, all_df_names, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-12T19:12:39.484632Z",
     "start_time": "2019-09-12T19:12:39.157117Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T15:17:42.307453Z",
     "start_time": "2019-10-10T15:17:42.080173Z"
    }
   },
   "outputs": [],
   "source": [
    "milad_df = all_df.loc[all_df[\"symbol\"]==\"خپارس\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T14:56:24.810071Z",
     "start_time": "2019-10-10T14:56:24.802193Z"
    }
   },
   "outputs": [],
   "source": [
    "# xtmp = pd.read_excel(xcelLocation + \"1380-2-3\" + \".xlsx\", header=[0], skiprows=[0,1])\n",
    "# xtmp = pd.read_excel(xcelLocation + \"1380-1-28\" + \".xlsx\", header=[0], skiprows=[0,1], na_values=[\"Infinity\"], convert_float=False)\n",
    "# xtmp.iloc[47]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
